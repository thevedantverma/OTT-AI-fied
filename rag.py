# -*- coding: utf-8 -*-
"""RAG

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HEB9xn5rGgMcfgpEUPZwNJlvE55oVd33
"""

!pip install langchain_community langchainhub chromadb langchain langchain-openai

# from langchain_core.prompts import ChatPromptTemplate

# template = """You are an assistant for question-answering tasks. \nUse the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. \nUse three sentences maximum and keep the answer concise.\nQuestion: {question} \nContext: {context} \nAnswer:"""
# prompt = ChatPromptTemplate.from_template(template)

pip install pypdf

!pip install --upgrade langchain langchain-community pypdf

!pip install pdfplumber langchain-community

import glob
from langchain_community.document_loaders import PDFPlumberLoader
folder_path = "/content/data"
def load_multiple_pdfs(folder_path):
    all_docs = []
    pdf_files = glob.glob(folder_path + "/*.pdf")

    for file in pdf_files:
        loader = PDFPlumberLoader(file)
        docs = loader.load()
        all_docs.extend(docs)

    return all_docs

pip install pdfplumber

docs = load_multiple_pdfs("/content/data")
print(len(docs))
print(docs[0])

from langchain_community.document_loaders import PyPDFLoader

loader = PyPDFLoader("/content/data/Netflix_OTT_Features.pdf")
docs = loader.load()

print(docs[:2])

from langchain_community.document_loaders import PyPDFLoader

loader = PyPDFLoader("/content/data/Netflix_OTT_Features.pdf")
docs = loader.load()

print(docs[:2])

!ls /content/data

from langchain_text_splitters import RecursiveCharacterTextSplitter

text_splitter = RecursiveCharacterTextSplitter(chunk_size = 1000, chunk_overlap = 200)
splits = text_splitter.split_documents(docs)

pip install langchain.text_splitter

print(splits[0])
print(splits[1])
print(splits[2])

print(len(splits))

#huggingface

!pip install sentence-transformers langchain_community transformers==4.30.2 --force-reinstall

!pip install sentence-transformers langchain_community langchain

!pip install sentence-transformers langchain-community pdfplumber chromadb

from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma

# HuggingFace Embedding model load
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)

vectorstore = Chroma.from_documents(
    documents=splits,
    embedding=embeddings
)

from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma

embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)

vectorstore = Chroma.from_documents(
    documents=splits,
    embedding=embeddings
)

retriever = vectorstore.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 3}
)

!pip install huggingface_hub

from huggingface_hub import login
login()







from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

model_name = "microsoft/phi-2"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    torch_dtype="auto"
)

hf_pipeline = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=300,
    do_sample=False,
    pad_token_id=tokenizer.eos_token_id,
    eos_token_id=tokenizer.eos_token_id     # IMPORTANT FIX
)

from langchain_community.llms import HuggingFacePipeline
llm = HuggingFacePipeline(pipeline=hf_pipeline)

def format_docs(docs):
    return "\n\n".join([d.page_content for d in docs])

!pip install langchain.prompts
!pip install langchain.shema

hf_pipeline = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=200,
    do_sample=False,
    pad_token_id=tokenizer.eos_token_id
)

from langchain_community.llms import HuggingFacePipeline
llm = HuggingFacePipeline(
    pipeline=hf_pipeline,
    model_kwargs={"max_new_tokens": 200}
)

prompt_text = prompt.format(
    context=format_docs(docs),
    question="Is document ka summary batao?"
)
print("\n--- Prompt Sent to LLM ---\n")
print(prompt_text)







from langchain_core.prompts import PromptTemplate

prompt = PromptTemplate(
    input_variables=["context", "question"],
    template="""
You are an AI assistant.

Use ONLY the PDF to answer the question.
If the answer is not in the context, say: "I don't know."

Context:
{context}

Question:
{question}

Answer:
"""
)

print("\n--- Raw LLM Output ---\n")
print(llm.invoke("Hello"))

docs = retriever.invoke("summary")
print("\n--- Retrieved Docs ---\n")
for d in docs:
    print(d.page_content[:300])
    print("-"*50)









from langchain.chains import RetrievalQA

qa_chain = RetrievalQA.from_chain_type(
    llm=llm,
    retriever=retriever,
    return_source_documents=True
)













print(vectorstore._collection.count())

print(vectorstore._collection.get())

retriever = vectorstore.as_retriever()

from langchain_openai import ChatOpenAI
llm = ChatOpenAI()

from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

def format_docs(docs):
  return "\n".join(doc.page_content for doc in docs)

from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

rag_chain = (
    {
        "context": retriever | format_docs,
        "question": RunnablePassthrough()
    }
    | prompt
    | llm
    | StrOutputParser()
)

response = rag_chain.invoke("Is document ka summary batao?")
print(response)

from langchain_core.runnables import RunnableLambda

def print_prompt(prompt_text):
  print("Prompt - ", prompt_text)
  return prompt_text

rag_chain_with_print = ({"context" : retriever | format_docs, "question" : RunnablePassthrough()}
             | prompt
             | RunnableLambda(print_prompt)
             | llm
             | StrOutputParser())

rag_chain.invoke("What is hero banner")

##REFINED###FINAL####

pip install langchain_community

pip install langchain-text-splitters

pip install langchain-core

import glob
from langchain_community.document_loaders import PDFPlumberLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma
from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain_community.llms import HuggingFacePipeline
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableLambda

pip install pdfplumber

def load_multiple_pdfs(folder_path="/content/data"):
    all_docs = []
    pdf_files = sorted(glob.glob(folder_path + "/*.pdf"))

    if not pdf_files:
        raise FileNotFoundError(f"No PDFs found in folder: {folder_path}")

    print("PDFs found:", pdf_files)

    for pdf in pdf_files:
        print("\nLoading:", pdf)
        loader = PDFPlumberLoader(pdf)
        docs = loader.load()
        all_docs.extend(docs)

    return all_docs


# Loading PDFs
docs = load_multiple_pdfs("/content/content
")

print("\nTotal PDF pages/documents loaded:", len(docs))
print("\nSample content (first 300 chars):")
print(docs[0].page_content[:300])

pip install chromadb

pip install sentence-transformers

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)

splits = text_splitter.split_documents(docs)

print(f"\nTotal chunks created: {len(splits)}")
print("\nChunk sample:")
print(splits[0].page_content[:300])


#Sentence Transformer Embeddings
embeddings = HuggingFaceEmbeddings(
    model_name="sentence-transformers/all-MiniLM-L6-v2"
)

print("\nEmbeddings model loaded.")


#Build Chroma vectorstore
vectorstore = Chroma.from_documents(
    documents=splits,
    embedding=embeddings,
    persist_directory=None
)

print("\nChroma vectorstore created.")



retriever = vectorstore.as_retriever(search_kwargs={"k": 3})

print("\nRetriever ready.")

model_name = "microsoft/phi-2"
print("Loading LLM:", model_name)


from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",
    torch_dtype="auto"
)

if tokenizer.eos_token_id is None:
    # fallback
    tokenizer.pad_token = tokenizer.eos_token or tokenizer.convert_ids_to_tokens([0])[0]

try:
    model.generation_config.pad_token_id = tokenizer.eos_token_id
    model.generation_config.eos_token_id = tokenizer.eos_token_id
except Exception as e:

    print("Warning setting generation_config:", e)

hf_pipeline = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=300,
    do_sample=False,
    pad_token_id=tokenizer.eos_token_id,
    eos_token_id=tokenizer.eos_token_id
)
print("HF text-generation pipeline ready.")

from langchain_community.llms import HuggingFacePipeline
llm = HuggingFacePipeline(pipeline=hf_pipeline, model_kwargs={"max_new_tokens": 300})
print("LangChain LLM wrapper ready (llm variable).")

from langchain_core.prompts import PromptTemplate

prompt = PromptTemplate(
    input_variables=["context", "question"],
    template="""
You are an AI assistant.

Use ONLY the document provided to answer the question.
If the answer is not found in the context, say "I don't know."

Context:
{context}

Question:
{question}

Answer:
"""
)

print("Prompt template ready.")



def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)


#RAG chain
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

rag_chain = (
    {
        "context": retriever | format_docs,
        "question": RunnablePassthrough()
    }
    | prompt
    | llm
    | StrOutputParser()
)

print("RAG chain is ready.")

###EXECUTION###

print("\n====================")
print("TEST 1: RETRIEVER")
print("====================")

sample_docs = retriever.invoke("summary")
for i, d in enumerate(sample_docs[:3]):
    print(f"\n--- Retrieved Doc #{i} (first 300 chars) ---\n")
    print(d.page_content[:300])
    print("-------------------------------------------")


print("\n====================")
print("TEST 2: PROMPT PREVIEW")
print("====================")

prompt_text = prompt.format(
    context=format_docs(sample_docs),
    question="What this document is all about?"
)

print("\n--- Prompt that will be sent to LLM (first 1000 chars) ---\n")
print(prompt_text[:1000])
print("\n-----------------------------------------------------------")


print("\n====================")
print("TEST 3: RAW LLM CHECK")
print("====================")

try:
    raw_output = llm.invoke("Hello! Please say hi.")
    print("\nRaw output from LLM:\n", raw_output)
except Exception as e:
    print("LLM error:", e)


print("\n====================")
print("TEST 4: FULL RAG QUERY")
print("====================")

response = rag_chain.invoke("What is a Spiderman") ##
print("\nRAG Response:\n", response)


print("\n====================")
print("TEST 5: PROMPT DEBUG VERSION")
print("====================")

from langchain_core.runnables import RunnableLambda

def print_prompt(x):
    print("\n\n=========== PROMPT SENT TO LLM ===========\n")
    print(x)
    print("\n===========================================\n")
    return x

rag_chain_debug = (
    {
        "context": retriever | format_docs,
        "question": RunnablePassthrough()
    }
    | prompt
    | RunnableLambda(print_prompt)
    | llm
    | StrOutputParser()
)

dbg_output = rag_chain_debug.invoke("What is Analytics, Feedback & Admin Tools?")##
print("\nDebug RAG Response:\n", dbg_output)

print("\n====================")
print("ALL TESTS COMPLETED.")
print("====================")













pip install streamlit sentence-transformers langchain langchain-community chromadb pdfplumber transformers

# WITH UI

!pip install --upgrade --force-reinstall blinker
!pip install blinker==1.7 --force-reinstall

pip install langchain-text-splitters

import gradio as gr
import glob
import tempfile

from langchain_community.document_loaders import PDFPlumberLoader
from langchain_text_splitters import RecursiveCharacterTextSplitter

from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain_community.vectorstores import Chroma

from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline
from langchain_community.llms import HuggingFacePipeline
from langchain_core.prompts import PromptTemplate

from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

print("‚úÖ PART 1 Loaded Successfully")

# ============================
# PART 2 ‚Äî PHI-2 LLM LOAD
# ============================

model_name = "microsoft/phi-2"
print(f"üöÄ Loading model: {model_name}")

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    device_map="auto",      # GPU use karega agar available
    torch_dtype="auto"      # Efficient dtype
)

# Set generation pipeline with proper stop tokens
hf_pipeline = pipeline(
    "text-generation",
    model=model,
    tokenizer=tokenizer,
    max_new_tokens=256,
    do_sample=False,
    pad_token_id=tokenizer.eos_token_id,
    eos_token_id=tokenizer.eos_token_id
)

# LangChain LLM wrapper
llm = HuggingFacePipeline(
    pipeline=hf_pipeline,
    model_kwargs={"max_new_tokens": 256}
)

print("‚úÖ PART 2: Phi-2 LLM Ready")

def build_vectorstore(pdf_files):
    temp_dir = tempfile.mkdtemp()
    docs = []

    # 1) Save uploaded PDFs temporarily + Load them
    for file in pdf_files:
        temp_path = f"{temp_dir}/{file.name}"

        # Save file
        with open(temp_path, "wb") as f:
            f.write(file.read())

        # Load with PDFPlumber
        loader = PDFPlumberLoader(temp_path)
        loaded_pages = loader.load()
        docs.extend(loaded_pages)

    # 2) Split text into chunks
    splitter = RecursiveCharacterTextSplitter(
        chunk_size=800,
        chunk_overlap=150
    )
    chunks = splitter.split_documents(docs)

    # 3) Embedding model
    embeddings = HuggingFaceEmbeddings(
        model_name="sentence-transformers/all-MiniLM-L6-v2"
    )

    # 4) Chroma vectorstore
    vectorstore = Chroma.from_documents(chunks, embeddings)

    # 5) Retriever
    retriever = vectorstore.as_retriever(search_kwargs={"k": 2})

    print("üìö PDFs Processed | üîç Vectorstore Ready | üéØ Retriever Ready")
    return retriever

prompt = PromptTemplate(
    input_variables=["context", "question"],
    template="""
Use ONLY the provided context to answer the question.
If the answer is not in the context, say "I don't know."

Context:
{context}

Question: {question}

Answer:
"""
)

# Format function for documents into plain text
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)


# RAG query function
def answer_question(question, retriever):
    if retriever is None:
        return "‚ö†Ô∏è Please upload & process PDFs first."

    # Build RAG chain dynamically
    rag_chain = (
        {
            "context": retriever | format_docs,
            "question": RunnablePassthrough()
        }
        | prompt
        | llm
        | StrOutputParser()
    )

    # Generate answer
    return rag_chain.invoke(question)

# # ============================
# # PART 5 ‚Äî GRADIO UI
# # ============================

# with gr.Blocks(title="RAG PDF QA (Phi-2)") as app:

#     gr.Markdown("## üìò RAG PDF Question Answering (Phi-2)\nUpload PDFs and ask questions!")

#     # -------- PDF Upload --------
#     pdfs = gr.File(label="Upload PDF files", file_types=["pdf"], file_count="multiple")
#     retriever_state = gr.State(None)
#     status_box = gr.Textbox(label="Status", interactive=False)

#     # Process PDFs Button
#     def load_pdfs(files):
#         if not files:
#             return None, "‚ö†Ô∏è Please upload at least one PDF."
#         retriever = build_vectorstore(files)
#         return retriever, "‚úÖ PDFs processed successfully. Vectorstore is ready!"

#     process_btn = gr.Button("Process PDFs")
#     process_btn.click(
#         load_pdfs,
#         inputs=[pdfs],
#         outputs=[retriever_state, status_box]
#     )

#     # -------- Question/Answer --------
#     question_box = gr.Textbox(label="Ask a Question")
#     answer_box = gr.Textbox(label="Answer")

#     ask_btn = gr.Button("Ask")

#     ask_btn.click(
#         answer_question,
#         # inputs=[question_box, retriever_state],
#         outputs=[answer_box]
#     )

# # Launch the Gradio app
# app.launch()

!kill -9 $(lsof -t -i:7860)

import gradio as gr

# Simple function to call your RAG chain
def rag_answer(question):
    if not question.strip():
        return "‚ùó Please enter a question."

    try:
        response = rag_chain.invoke(question)
        return response
    except Exception as e:
        return f"‚ö†Ô∏è Error: {str(e)}"


# Minimal Gradio UI
with gr.Blocks(title="Minimal RAG Chat UI") as demo:

    gr.Markdown("## üß† Minimal RAG Chat (Using Pre-loaded PDFs)")
    gr.Markdown("Ask any question based on your processed document knowledgebase.")

    question_box = gr.Textbox(
        label="Your Question",
        placeholder="Ask something...",
        lines=2
    )

    answer_box = gr.Textbox(
        label="RAG Answer",
        lines=10
    )

    ask_btn = gr.Button("Ask")

    ask_btn.click(
        fn=rag_answer,
        inputs=[question_box],
        outputs=[answer_box]
    )

demo.launch()

